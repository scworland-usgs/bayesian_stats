---
title: "SR chapter 3: sampling"
author: "scworland@usgs.gov"
date: "2017"
output:
  pdf_document:
    toc: yes
---

```{r, echo=F}
knitr::opts_chunk$set(fig.width=5,fig.height=3,fig.align = 'center', warning=F, message=F,cache=F)
library(pacman)
pacman::p_load(coda,mvtnorm,devtools,rethinking,ggplot2,gridExtra,ggthemes,dplyr,magrittr,reshape2,xtable)

#install_github("rmcelreath/rethinking")
```

## SR Chapter 3: Sampling

This chapter deals with sampling from a posterior distribution. This is something I am already familiar with so I just touch on the examples here.

### Basic sampling

First create a posterior distribution using the exact code above:

```{r}
p_grid <- seq(from=0, to=1, length.out=1000)
prior <- rep(1, 1000)
likelihood <- dbinom(6, size=9, prob=p_grid)
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
```

Now we can draw 100,000 samples from the posterior distribution using the `sample` function

```{r}
samples <- sample(p_grid, prob=posterior, size=1e5, replace=T)
```

```{r, echo=F, cache=F}
ggplot(data=data.frame(samples), aes(1:length(samples),samples)) + 
  geom_point(alpha=0.1) + theme_bw() + 
  stat_density2d(aes(fill = ..level..), geom="polygon",alpha=0.1,bins = 50) +
  labs(y="proportion of water", x="sample number") + guides(fill=F) + ylim(0,1)
```

### Summarizing samples

The next step is to ask questions about the posterior. For example, what is the posterior probability that the proportion of water is less than 0.5? Because we have a grid (discrete values), we can just sum all the probabilities where `p_grid` is less than 0.5:

```{r}
sum(posterior[p_grid < 0.5])
```

For a single parameter, this is simple! This simplicity will not hold once we move to multiple parameters. One way that does generalize well is to just divide the number of samples less than 0.5 by the total number of samples:

```{r}
sum(samples < 0.5)/length(samples)
```


We are just approximating the probability using samples from the posterior distribution, but as we see, it gets pretty close. We can ask any question this way:

```{r}
sum(samples > 0.5 & samples < 0.75)/length(samples)
```


There is a 60% chance that $p$ falls between 0.5 and 0.75. What if we wanted to ask the opposite? What values of $p$ bound the middle 80% of the density?

```{r}
quantile(samples, c(0.1,0.9))
```

```{r, echo=F, warning=F}
samp_dens <- with(density(samples), data.frame(x,y)) %>%
  mutate(y=y/sum(y))
  
low = quantile(samples, c(0.1,0.9))[1]
high = quantile(samples, c(0.1,0.9))[2]

ggplot(data=samp_dens, aes(x,y)) + 
  geom_line() + theme_bw() + labs(x="samples",y="density") +
  geom_area(aes(x = ifelse(x>low & x<high , x, 0)),alpha=0.5) +
  scale_y_continuous(limits = c(0, max(samp_dens$y))) +
  scale_x_continuous(limits = c(0.01, 1.1))
```

So 80% of the probability lies between $p$ values of 0.45 and 0.81. These are referred to as *percentile intervals* (PI) and work well if the posterior distribution isn't too asymmetrical. Let's create an example where the PI doesn't really reflect which parameters are consistent with the data, where we toss the globe three times and three waters.

```{r}
p_grid <- seq(from=0, to=1, length.out=1000)
prior <- rep(1, 1000)

# 3 tosses, 3 waters
likelihood <- dbinom(3, size=3, prob=p_grid) 
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)

samples <- sample(p_grid, prob=posterior, size=1e4, replace=T)
```

Let's look at the 50th percentile:

```{r}
# 50th PI
quantile(samples, c(0.25,0.75))

# 50th PI using rethinking function
PI(samples, prob=0.5)
```

```{r, echo=F, warning=F}
samp_dens <- with(density(samples), data.frame(x,y)) %>%
  mutate(y=y/sum(y))
  
low = PI(samples, prob=0.5)[1]
high = PI(samples, prob=0.5)[2]

ggplot(data=samp_dens, aes(x,y)) + 
  geom_line() + theme_bw() + labs(x="samples",y="density") +
  geom_area(aes(x = ifelse(x>low & x<high , x, 0)),alpha=0.5) +
  scale_y_continuous(limits = c(0, max(samp_dens$y))) +
  scale_x_continuous(limits = c(0.01, 1.1))
```

This excludes the most probable parameter values (near 1 for this example), so obviously PI isn't what we are interested in. What we want to know is _what is the narrowest interval that contains the probability mass we are interested in_. This is referred to as the highest posterior density interval (HPDI) and can be found using the `HPDI` function:

```{r}
# HPDI from the rethinking package
HPDI(samples, prob=0.5)
```

```{r, echo=F, warning=F}
samp_dens <- with(density(samples), data.frame(x,y)) %>%
  mutate(y=y/sum(y))
  
low = HPDI(samples, prob=0.5)[1]
high = HPDI(samples, prob=0.5)[2]

ggplot(data=samp_dens, aes(x,y)) + 
  geom_line() + theme_bw() + labs(x="samples",y="density") +
  geom_area(aes(x = ifelse(x>low & x<high , x, 0)),alpha=0.5) +
  scale_y_continuous(limits = c(0, max(samp_dens$y))) +
  scale_x_continuous(limits = c(0.01, 1.1))
```


The HPDI captures the parameters with the highest posterior probability and it's also much narrower than the PI. 

### Point estimates

We can also calculate point estimates from the posterior distribution:

```{r}
c(mean(samples), median(samples), chainmode(samples))
```

```{r, echo=F, warning=F}
samp_dens <- with(density(samples), data.frame(x,y)) %>%
  mutate(y=y/sum(y))

ggplot(data=samp_dens, aes(x,y)) + 
  geom_line() + theme_bw() + labs(x="samples",y="density") +
  scale_y_continuous(limits = c(0, max(samp_dens$y))) +
  scale_x_continuous(limits = c(0.01, 1.1)) + 
  geom_vline(xintercept=mean(samples),linetype="dashed",color="red") +
  geom_vline(xintercept=chainmode(samples),linetype="dashed",color="blue") + 
  geom_vline(xintercept=median(samples),linetype="dashed") + 
  annotate("label",x=mean(samples), y=0.002, label="mean",color="red") +
  annotate("label",x=median(samples), y=0.001, label="median") +
  annotate("label",x=chainmode(samples), y=0.003, label="MAP",color="blue")
  
```


It is better to just report the whole posterior distribution, but if we are required to report a single value, then we can use a loss function to decide which value we want to report. A loss function just calculates the "cost" associated with choosing a particular value of a parameter. For example, if we choose 0.5 as our parameter, the cost of that choice is:

```{r}
sum(posterior*abs(0.5-p_grid))
```

That is, we just take the absolute value of the difference between our choice and every other possible value and weight each difference by the probability of the parameter values in `p_grid`. If a parameter value has high probability, we want to be sure to penalize the difference between that value and our guess. The absolute loss, $d-p$, corresponds to the the median, and the squared loss, $(d-p)^2$, corresponds to the mean.

```{r}
absolute.loss <- sapply(p_grid, function(d) sum(posterior*abs(d-p_grid)))
squared.loss <- sapply(p_grid, function(d) sum(posterior*(d-p_grid)^2))

p_grid[which.min(absolute.loss)]
p_grid[which.min(squared.loss)]
```

```{r, echo=F}
loss.df <- data.frame(p_grid,absolute.loss,squared.loss) %>%
  melt(., id.vars="p_grid")

ggplot(loss.df) + geom_line(aes(x=p_grid,y=value,color=variable)) + theme_bw() +
  labs(x="parameter value", y="loss value", color="loss function") +
  theme(legend.position=c(0.7,0.7)) +
  geom_point(aes(x=p_grid[which.min(absolute.loss)], 
                 y = absolute.loss[which.min(absolute.loss)]), shape=21) +
  geom_point(aes(x=p_grid[which.min(squared.loss)], 
                 y = squared.loss[which.min(squared.loss)]),shape=21)
```


### Simulating observations

We often want to simulate our data using the predictions from our model (often called "dummy data"). Let's start by looking at the probability of every possible out come from two tosses of the globe (0 water, 1 water, or 2 waters), and use the real proportion of water on earth (0.7).

```{r}
dbinom(0:2, size=2, prob=0.7)
```

The output is saying that the probability of seeing zero waters if p = 0.7 is 0.09, seeing 1 water is 0.42, and 2 waters 0.49. Now let's sample from this distribution.

```{r}
rbinom(10, size=2, prob=0.7)
```

Each value in the output provides the number of waters seen if the globe was tossed 3 times. The 10 in the function just simulates the 2 tosses 10 times. If we run this a bunch of times, the proportion of 0s, 1s, and 2s should be the same as the analytical values we got from using `dbinom`.

```{r}
n=1000
table(rbinom(n, size=2, prob=0.7))/n
```

The above examples were for using only two samples (or tosses). Let's look at it for our 9 toss example.

```{r}
n=1e5
w <- rbinom(n, size=9, prob=0.7)
```

```{r, echo=F}
d <- data.frame(table(w)/n)

# plot
ggplot(d) + geom_bar(aes(w, Freq),stat="identity", width=0.05) +
  theme_bw() + labs(x="predicted number of waters",y="Frequency") 
```

### Simulating observations for the globe model

In reality, we really want our model to propagate the uncertainty of the parameters into the model predictions. For the globe tossing example, we have two types uncertainty (we could come up with more, but below are the two big ones). 

1. sampling uncertainty: although we know that water is more likely, we do not know what the next toss we be
2. parameters uncertainty: there is some level of uncertainty around $p$ for more real world problems. 

Let's first recreate our posterior distribution and sample from it:

```{r}
p_grid <- seq(from=0, to=1, length.out=1000)
prior <- rep(1, 1000)

# The likelihood of getting 6 waters from 9 tosses
# for each possible value of p defined in p_grid
likelihood <- dbinom(6, size=9, prob=p_grid) 
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)

# 10,000 samples from the posterior distribution
samples <- sample(p_grid, prob=posterior, size=1e4, replace=T)
```

Let's plot our samples from the posterior:

```{r, echo=F}
samp_dens <- with(density(samples), data.frame(x,y)) %>%
  mutate(y=y/sum(y))

ggplot(data=samp_dens, aes(x,y)) + 
  geom_line() + theme_bw() + labs(x="samples from posterior",y="density") 
```

Now, lets generate 100,000 random binomial samples for the globe example, using the MAP (=0.676) from our distribution as the probability:

```{r}
n=1e5

# Create predictions using MAP of samples
w <- rbinom(n, size=9, prob=chainmode(samples))
```

recall that `w` is a vector of values between 0 and 9, representing a hypothetical "9-toss globe experiment" using a fixed value for $p$, where $p=0.676$, the MAP point estimate from the samples vector. 

How can we use our posterior distribution to propogate parameter uncertainty into our predictions? Think about what we have in the `samples` vector. We have a bunch (10,000) possible values of $p$ that are repeated in proportion to how much "we believe" (based off our likelihood and prior) each value is the "true" value. We can just replace a fixed value, like the MAP, with the samples vector when making our predictions. Let's do it, and then think about what it is giving us.

```{r}
n=1e5
w <- rbinom(n, size=9, prob=samples)
```

McElreath says is better than I could, "For each sampled value, a random binmoial observation is generated. Since the sampled values appear in proportion to their posterior probabilities, the resulting simulated observations are averaged over the posterior." Let's look at our predictions both using several point estimates (MAP, mean, median), and the samples vector:

```{r, echo=F}
n=1e5
w_map <- rbinom(n, size=9, prob=chainmode(samples))
w_mean <- rbinom(n, size=9, prob=mean(samples))
w_median <- rbinom(n, size=9, prob=median(samples))

d <- data.frame(table(w)/n) %>% 
  rename(posterior=Freq, p = w) %>%
  mutate(MAP = table(w_map)/n,
         mean = table(w_mean)/n,
         median = table(w_median)/n) %>%
  melt(., id.vars="p")

# plot
ggplot(d, aes(p,value,fill=variable)) + 
  geom_bar(color="black",stat="identity", width=0.35,position=position_dodge(width=0.5)) +
  labs(x="predicted number of waters",y="Frequency") + theme_bw() +
  theme(legend.position=c(0.2,0.65))
```

Note that the point estimates predict the values 5, 6, and 7 with a higher frequency than when we average over the entire posterior. Also note that using the entire posterior predicts 0, 1, 2, 3, and 9 with a higher frequency than the point estimates. We are effectively saying, "we don't have enough data to be as confident as our point estimates might lead us to believe." Remember we only have 9 data observations! We might also want to compare things like the correlation structure between the data and predictions (e.g., how many switches do we have in our data, WLWWWLWLW = 6 switches, and compare that to our predictions). This is one way to address assumptions of our model, like independence of the tosses.

### Homework

**(3M1)**

Globe tossing example had turned out to be 8 water in 15 tosses:

```{r}
p_grid <- seq(from=0, to=1, length.out=1000)
prior <- rep(1, 1000)

# The likelihood of getting 8 waters from 15 tosses
# for each possible value of p defined in p_grid
likelihood <- dbinom(8, size=15, prob=p_grid) 
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
plot( posterior ~ p_grid , type="l" )
```

**(3M2)**

Sample posterior 10,000 times and extract 90% HPDI:

```{r}
# 10,000 samples from the posterior distribution
samples <- sample(p_grid, prob=posterior, size=1e4, replace=T)

# 90% HPDI
HPDI(samples, 0.9)
```

**(3M3)**

Construct a posterior predictive check using the entire posterior. What is the probability of observing 8 water in 15 tosses? 

```{r, fig.width=3, fig.height=3}
# generate 10,000 predictions
w <- rbinom(1e4, 15, samples)
simplehist(w)
```

```{r, echo=F, results='asis'}
d <- data.frame(table(w)/1e4) %>%
  rename("# of waters" = w, "Frequency"=Freq)

d.tab <- xtable(d)
align(d.tab) <- c("c","c","c")
print(d.tab,include.rownames=F,comment = F)
```

**(3M4)**

Use the posterior distribution to calculate the probability of observing 6 waters in 9 tosses:

```{r, fig.width=3, fig.height=3}
# generate 10,000 predictions
w_old <- rbinom(1e4, 9, samples)
simplehist(w_old)
```

```{r, echo=F, results='asis'}
d <- data.frame(table(w)/1e4) %>%
  rename("# of waters" = w, "Frequency"=Freq)

d.tab <- xtable(d)
align(d.tab) <- c("c","c","c")
print(d.tab,include.rownames=F,comment = F)
```

**(3M5)**

Step through 3M1-3M4 but using a prior that is zero for $p \leq$ 0.5 and constant above 0.5

```{r}
p_grid <- seq(from=0, to=1, length.out=1000)
prior <- ifelse(p_grid <= 0.5, 0, 1) # truncated prior

# The likelihood of getting 8 waters from 15 tosses
# for each possible value of p defined in p_grid
# using a truncated prior
likelihood <- dbinom(8, size=15, prob=p_grid) 
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)

# 10,000 samples from the posterior distribution
samples <- sample(p_grid, prob=posterior, size=1e4, replace=T)
```

Let's plot the samples from the new posterior:

```{r, echo=F}
samp_dens <- with(density(samples), data.frame(x,y)) %>%
  mutate(y=y/sum(y))

ggplot(data=samp_dens, aes(x,y)) + 
  geom_line() + theme_bw() + labs(x="samples from posterior",y="density") 
```

Notice the almost zero probability below and 0.5 (there is a little bit due to sampling error). We can also calculate the new 90% HPDI:

```{r}
# 90% HPDI
HPDI(samples, 0.9)
```

Recalculate the posterior predictive check and plot both predicted frequencies on the same plot:

```{r, fig.width=3, fig.height=3}
# generate 10,000 predictions
w2 <- rbinom(1e4, 15, samples)
simplehist(w2)
```

```{r, echo=F}

d2 <- data.frame(table(w2)/1e4) %>%
  rename(truncated=Freq, p = w2) 

d <- data.frame(table(w)/1e4) %>% 
  rename(uniform=Freq, p = w) %>%
  left_join(d2, by = "p") %>%
  mutate(truncated = ifelse(is.na(truncated),0,truncated)) %>%
  melt(., id.vars="p") %>%
  mutate(p=as.numeric(p))

# plot
ggplot(d, aes(p,value,fill=variable)) + 
  geom_bar(color="black",stat="identity", width=0.35,position=position_dodge(width=0.5)) +
  scale_x_continuous(breaks=0:15) +
  labs(x="predicted number of waters",y="Frequency") + theme_bw() +
  geom_vline(xintercept=0.7*15, linetype="dashed") +
  theme(legend.position=c(0.2,0.65))
```

The dashed line represents the "true" value of 0.7. Notice how the informative, truncated prior allocated more probability to the predicted values closer to the true value of 10.5 waters out of 15.


**(3H)**

For these exercises we're going to be using a dataset from the `rethinking` package. The data represents the gender (male=1, female=0) of officially reported 1st and 2cd born children in 100 two-child families. The table below has only the first 10 values

```{r}
data(homeworkch3)
```

```{r, echo=F, results='asis'}
births <- data.frame(first.child = birth1, second.child = birth2)
births.tab <- xtable(births[1:10,],digits=c(0,0,0))
align(births.tab) <- c("c","c","c")
print(births.tab,include.rownames=T,comment = F)
```

**(3H1)**

Use grid approximation to calculate the posterior distribution of the probability of a birth being a boy assuming a uniform prior. Which parameter value maximizes the posterior? We need to first figure out how many boys there were:

```{r}
nboys <- sum(birth1) + sum(birth2) # number of boys
total <- length(birth1) + length(birth2) # number of births
```


```{r}
n=1000
p_grid <- seq(from=0, to=1, length.out=n)
prior <- rep(1, n) # uniform prior

# The likelihood of getting 111 boys from 22 births
# for each possible value of p defined in p_grid
# using a uniform prior
likelihood <- dbinom(nboys, size=total, prob=p_grid) 
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)

MAP <- p_grid[which.max(posterior)]
MAP
```

```{r, echo=F}
ggplot() + geom_line(aes(p_grid,posterior)) + theme_bw() +
  geom_vline(xintercept=MAP, linetype="dashed")
```

**(3H2)**

Draw 10,000 samples from the posterior and calculate the 50%, 89%, and 97% HPDIs:

```{r}
# 10,000 samples from the posterior distribution
samples <- sample(p_grid, prob=posterior, size=1e4, replace=T)

# 50%, 89%, and 97% HPDIs
HPDI(samples, 0.5)
HPDI(samples, 0.89)
HPDI(samples, 0.97)
```

```{r, echo=F}
low50 = HPDI(samples, 0.5)[1]
high50 = HPDI(samples, 0.5)[2]

low89 = HPDI(samples, 0.89)[1]
high89 = HPDI(samples, 0.89)[2]

low97 = HPDI(samples, 0.97)[1]
high97 = HPDI(samples, 0.97)[2]

ggplot() + geom_line(aes(p_grid,posterior)) + theme_bw() +
  labs(x="samples",y="density") +
  geom_area(aes(x = ifelse(p_grid>low50 & p_grid<high50, p_grid, 0),y=posterior),alpha=0.9) +
  geom_area(aes(x = ifelse(p_grid>low89 & p_grid<high89, p_grid, 0),y=posterior),alpha=0.6) +
  geom_area(aes(x = ifelse(p_grid>low97 & p_grid<high97, p_grid, 0),y=posterior),alpha=0.3) +
  scale_y_continuous(limits = c(0, max(posterior))) +
  scale_x_continuous(limits = c(MAP-0.15, MAP+0.15)) +
  annotate("text",x=0.47, y=0.002, label="97%") +
  annotate("text",x=0.50, y=0.006, label="89%") +
  annotate("text",x=0.53, y=0.011, label="50%") 
```

**(3H3)**

Simulate 10,000 births using our model and compare the predicted to the actual.

```{r}
# generate 10,000 predictions
b <- rbinom(1e4, total, samples)
```

```{r, echo=F}
d <- data.frame(table(b)/1e4) %>%
  mutate(b=as.numeric(as.character(b)))

# plot
ggplot(d) + geom_bar(aes(b, Freq),stat="identity", width=0.2) +
  theme_bw() + labs(x="predicted number of boys",y="Frequency") +
  scale_x_continuous(breaks=seq(75,150,5), labels=seq(75,150,5)) +
  geom_vline(xintercept=nboys,linetype="dashed", color="dodgerblue", size=1)
```

The blue line is the actual number of boys from our dataset. The model did a pretty good job. 

**(3H4)**

Now predict only 100 births (representing just the first borns), and compare the predicted values to the actual number of boys in the first borns group. This should stretch the model a bit more.

```{r}
# generate 10,000 predictions
b <- rbinom(1e4, length(birth1), samples)
```

```{r, echo=F}
d <- data.frame(table(b)/1e4) %>%
  mutate(b=as.numeric(as.character(b)))

# plot
ggplot(d) + geom_bar(aes(b, Freq),stat="identity", width=0.2) +
  theme_bw() + labs(x="predicted number of boys",y="Frequency") +
  scale_x_continuous(breaks=seq(30,80,5), labels=seq(30,80,5)) +
  geom_vline(xintercept=sum(birth1),linetype="dashed", color="dodgerblue", size=1)
```

The blue line is the actual number of boys from the first born column in our dataset. The model thinks there should be more boys than there actually are. This shows why a point estimate could be problematic, and why using density intervals (and judiciously reporting that any value within the interval is credible) can still capture the "correct" value. 

```{r}
births_after_girls <- birth2[birth1==0]
births_after_girls_sim <- rbinom(1e4, length(births_after_girls), samples)
```


```{r, echo=F}
d <- data.frame(table(births_after_girls_sim)/1e4) %>%
  mutate(b=as.numeric(as.character(births_after_girls_sim)))

# plot
ggplot(d) + geom_bar(aes(b, Freq),stat="identity", width=0.2) +
  theme_bw() + labs(x="predicted number of boys when first birth was a girl",y="Frequency") +
  scale_x_continuous(breaks=seq(0,40,5), labels=seq(0,40,5)) +
  geom_vline(xintercept=sum(births_after_girls),linetype="dashed", color="dodgerblue", size=1)
```
