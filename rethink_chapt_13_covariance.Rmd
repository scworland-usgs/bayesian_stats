---
title: "SR chapter 13: Covariance"
author: "scworland@usgs.gov"
date: "2017"
output:
  pdf_document:
    toc: yes
    toc_depth: 4
header-includes: 
  - \usepackage{tikz}
  - \usepackage{pgfplots}
---

```{r, echo=F}
knitr::opts_chunk$set(fig.width=6,fig.height=3,fig.align = 'center', warning=F, message=F,cache=F)
library(pacman)
pacman::p_load(coda,mvtnorm,devtools,rethinking,ggplot2,gridExtra,ggthemes,dplyr,magrittr,reshape2,xtable,purrr,MASS)

#install_github("rmcelreath/rethinking",ref="Experimental")
```

## Modeling covariance

When there are multiple varying effects (i.e., varying slopes and intercepts), we need to account for the covariance in the parameters. This arises naturally from the data. For example, imagine meausring wait times at two cafes during the morning and the afternoon. A very popular cafe would have a long wait time in the morning (large intercept) but likely a much smaller wait time in the afternoon (large slope), whereas a less popular cafe would have less of a difference between morninig and afternoon (smaller slope) with smaller wait times overall (smaller intercept). Hence the parameters are correlated for a given cafe. If we can pool information across parameter types, we can gain information. With the cafe example, knowing something about the morning (e.g., long wait time) would allow us to learn something about the afternoon (i.e., the slope) due to the covariance. 

In order to pool information across slopes and intercepts, we must also model the covariance via a joint multivariate gaussian (MVN) distribution for all the varying effects. The MVN distribution will allow us to describe the variation within and the covariation among the different varying effects. **Varying intercepts have variation, varying slopes have variation, and intercepts and slopes covary.**

### simulate example

We will simulate the cafe example from above.

```{r}
a <- 3.5       # average morning wait time
b <- -1        # average difference in afternoon wait time
sigma_a <- 1   # std dev in intercepts
sigma_b <-0.5  # std dev in slopes
rho <- -0.7    # correlation between a's and b's
```

We can now build a 2-d MVN to simulate samples. We first need a vector of means, and 2x2 matrix of variance in covariances:

$$
\begin{aligned}
N(\mu, \Sigma)
&\sim  N
\begin{bmatrix}
\begin{pmatrix}
\alpha\\
\beta
\end{pmatrix}\!\!,&
\begin{pmatrix}
\sigma_\alpha^2 & \sigma_\alpha\sigma_\beta\rho\\
\sigma_\alpha\sigma_\beta\rho & \sigma_\beta^2 
\end{pmatrix}
\end{bmatrix}
\end{aligned}
$$

```{r}
# prepare data
Mu <- c(a,b) # vector of means
sigmas <- c(sigma_a,sigma_b) # standard deviations
Rho <- matrix(c(1,rho,rho,1), ncol=2) # correlation matrix
Sigma <- diag(sigmas) %*% Rho %*% diag(sigmas) # covariance matrix

# simulate cafes
set.seed(5) # set seed
N <- 20 # number of cafes
vary_effects <- mvrnorm(N, Mu, Sigma) #simulate intercepts and slopes

# separate the parameters
cafe_params <- data.frame(a = vary_effects[,1],
                          b = vary_effects[,2])
```

```{r, echo=F}
ggplot(cafe_params) + geom_point(aes(a,b)) +
  labs(x=expression(alpha), y = expression(beta)) +
  stat_ellipse(aes(a,b),level=0.15,alpha=0.9) +
  stat_ellipse(aes(a,b),level=0.35,alpha=0.7) +
  stat_ellipse(aes(a,b),level=0.55,alpha=0.6) +
  stat_ellipse(aes(a,b),level=0.75,alpha=0.4) +
  stat_ellipse(aes(a,b),level=0.95,alpha=0.2) +
  theme_bw() 
```


Now we can simute wait times using this data:

```{r}
N_visits <- 10
afternoon <- rep(0:1,N_visits*N/2)
cafe_id <- rep(1:N, each=N_visits)
mu <- cafe_params$a[cafe_id] + cafe_params$b[cafe_id]*afternoon
sigma <- 0.5 #std dev within cafes
wait <- rnorm(N_visits*N,mu,sigma)
d <- data.frame(cafe=cafe_id, afternoon=afternoon, wait=wait)

head(d,12)
```

We can now model this data and see if we can recapture the known parameters. 

$$
\begin{aligned}
w_i &\sim Normal(\mu_i,\sigma) \\
\mu_i &= \alpha_{cafe[i]} + \beta_{cafe[i]}A_i\\
\begin{bmatrix}
\alpha_{cafe}\\
\beta_{cafe}
\end{bmatrix} &\sim MVN \left( \begin{bmatrix}
\alpha\\
\beta
\end{bmatrix},S \right)\\ 
S &= \begin{pmatrix}
\sigma_\alpha & 0\\
0 & \sigma_\beta 
\end{pmatrix} R
\begin{pmatrix}
\sigma_\alpha & 0\\
0 & \sigma_\beta 
\end{pmatrix}\\
\alpha &\sim Normal(0,10) \\
\beta &\sim Normal(0,10) \\
\sigma &\sim HalfCauchy(0,1) \\
\sigma_\alpha &\sim HalfCauchy(0,1) \\
\sigma_\beta &\sim HalfCauchy(0,1) \\
R &\sim LKJcorr(2)
\end{aligned}
$$

The third line defines the population of varying slopes and intercepts. It states that each cafe has an $\alpha_{cafe}$ and $\beta_{cafe}$ with a prior distribution defined by the two dimensional Gaussian dsitribution with means $\alpha$ and $\beta$ and covariance matrix **S**. This will adaptively regularize the individual slopes, intercepts, and the correlation among them. Finally, we assign a LKJ prior to $\rho$. The LKJ prior is slight regularizing prior for correlations. The distribution is controlled by a single parameter, $\eta$, which controls how skeptical the prior is of large correlations in the matrix. 

```{r,cache=T, results='hide'}
m13.1 <- map2stan(
  alist(
    wait ~ dnorm(mu, sigma),
    mu <- a_cafe[cafe] + b_cafe[cafe]*afternoon,
    c(a_cafe,b_cafe)[cafe] ~ dmvnorm2(c(a,b),sigma_cafe,Rho),
    a ~ dnorm(0,10),
    b ~ dnorm(0,10),
    sigma_cafe ~ dcauchy(0,2),
    sigma ~ dcauchy(0,2),
    Rho ~ dlkjcorr(2)
  ), 
  data=d,
  iter=5000,warmup=2000,chains=2,cores=2)
```

The model at least learned that there is negative correlation between slopes and intercepts:

```{r}
post <- extract.samples(m13.1)
summary(post$Rho[,1,2])
```

The book goes into some other stuff, but I jump straight to the non-centered parameterization on page 408.

## Non-centered parameterization and Cholesky Decomposition

When modeling the prior distribution of the covariance between parameters, we can increase the efficiency of our MCMC sampler by factoring the covariance matrix into the product of simpler matrices and assigning priors to those components. Signs of inefficient sampling are low effective samples, high rhat values, and divergent iteration warnings. We can reparameterize the model to use a *non-centered parameterization* (this is a horrible name because it bascially involves standardizing...). Just like we move $\mu$ to the linear model, we can also move $\sigma$ to the linear model:

$$
\begin{aligned}
&y \sim Normal(\mu,\sigma) \\ 
& is ~ equivalent~to \\ 
&y=\mu + z\sigma ~ where, ~ z \sim Normal(0,1)
\end{aligned}
$$
This leaves only a multivariate Gaussian prior, with means at zero and and standard deviations of 1. The prior is now bascially just a distribution of correlated z-scores. We just have to remember to multiply the varying intercept and slope by a scale parameter:

$$
\begin{aligned}
y &\sim Normal(\mu,\sigma) \\ 
\mu &= \alpha + \alpha_{group[i]}\sigma_{group,1} + (\beta + \beta_{group[i]}\sigma_{group,2})*x_1   
\end{aligned}
$$
```{r,eval=F}
m <- alist(
  y ~ dnorm(mu , sigma),
  mu <- a + zaj[group]*sigma_group[1] + 
    (b + zbj[group]*sigma_group[2])*x,
  c(zaj,zbj)[group] ~ dmvnorm(0 , Rho_group),
  a ~ dnorm(0, 10),
  b ~ dnorm(0, 1),
  sigma ~ dcauchy(0, 1),
  sigma_group ~ dcauchy(0, 1),
  Rho_group ~ dlkjcorr(2)
), data=d,
start=list(sigma_group=c(1,1,1)),
constraints=list(sigma_group="lower=0"),
types=list(Rho_group="corr_matrix"),
iter=5000,warmup=1000, chains=3,cores=3)
```

The `start`, `contraints`, and `types` lists are needed to inform stan of the proper dimension and constraints on the parameters. 

All that is left in the covariance prior above is the correlation matrix `Rho_group`. We can extract these using a **Cholesky decomposition**: allows us to represent a square correlation matrix such that $R=LL^T$.

```{r}
# correlation matrix
Omega <- rbind(c(1, 0.3, 0.2),
               c(0.3, 1, 0.1),
               c(0.2, 0.1, 1))

# cholesky decomposition
c <- chol(Omega)

# recreate Omega
t(c) %*% c
```

\newpage

In summary,

**For the LKJ prior setup, the parts are**:

1. A diagonal matrix of standard deviations --> gets priors on each sigma

2. A correlation matrix  --> gets the LKJ_cor prior

**For the Cholesky prior setup, the parts are**:

1. A diagonal matrix of standard deviations --> gets priors on each sigma

2. A lower triangular matrix (L) that is the Cholesky decomposition of the correlation matrix (R), i.e., L%*%t(L)=R --> here L gets a "Cholesky prior" in stan  (cholesky_factor_cor)

